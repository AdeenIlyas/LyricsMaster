{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4840139,"sourceType":"datasetVersion","datasetId":2805070}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:10:25.300088Z","iopub.execute_input":"2025-05-02T10:10:25.300657Z","iopub.status.idle":"2025-05-02T10:10:25.585965Z","shell.execute_reply.started":"2025-05-02T10:10:25.300626Z","shell.execute_reply":"2025-05-02T10:10:25.585237Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/genius-song-lyrics-with-language-information/song_lyrics.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"os.environ[\"UNSLOTH_DISABLE_CUSTOM_LOSS\"] = \"1\"\nos.environ[\"UNSLOTH_DISABLE_TRITON\"] = \"1\"\nos.environ[\"TRITON_CACHE_DIR\"] = \"/tmp/triton_cache\"\nos.environ[\"CCL_DISABLE_TRITON\"] = \"1\"\nos.environ[\"DISABLE_UNSLOTH_TRITON\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:10:25.587057Z","iopub.execute_input":"2025-05-02T10:10:25.587347Z","iopub.status.idle":"2025-05-02T10:10:25.591281Z","shell.execute_reply.started":"2025-05-02T10:10:25.587331Z","shell.execute_reply":"2025-05-02T10:10:25.590748Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%pip install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:10:25.591849Z","iopub.execute_input":"2025-05-02T10:10:25.592070Z","iopub.status.idle":"2025-05-02T10:13:28.746647Z","shell.execute_reply.started":"2025-05-02T10:10:25.592050Z","shell.execute_reply":"2025-05-02T10:13:28.745897Z"}},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2025.4.4-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2025.4.4 (from unsloth)\n  Downloading unsloth_zoo-2025.4.4-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.5.1+cu124)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes (from unsloth)\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.1)\nRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.5.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.3.0)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.30.2)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.20.1+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.16)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (14.0.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.4.4->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.4.4->unsloth) (11.1.0)\nCollecting msgspec (from unsloth_zoo>=2025.4.4->unsloth)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\nCollecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.1.0)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.19.0)\nCollecting hf-xet>=0.1.4 (from huggingface_hub[hf_xet]>=0.30.0->unsloth_zoo>=2025.4.4->unsloth)\n  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\nDownloading unsloth-2025.4.4-py3-none-any.whl (218 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.6/218.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.4.4-py3-none-any.whl (129 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.19-py3-none-any.whl (124 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, hf-xet, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu124\n    Uninstalling torch-2.5.1+cu124:\n      Successfully uninstalled torch-2.5.1+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu124\n    Uninstalling torchvision-0.20.1+cu124:\n      Successfully uninstalled torchvision-0.20.1+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0 which is incompatible.\ntorchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.7.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 fsspec-2024.12.0 hf-xet-1.1.0 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 trl-0.15.2 tyro-0.9.19 unsloth-2025.4.4 unsloth_zoo-2025.4.4 xformers-0.0.30\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import kagglehub\nfrom kagglehub import KaggleDatasetAdapter\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport os\nfrom unsloth import FastLanguageModel\nimport torch\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:13:28.748347Z","iopub.execute_input":"2025-05-02T10:13:28.748663Z","iopub.status.idle":"2025-05-02T10:14:00.166304Z","shell.execute_reply.started":"2025-05-02T10:13:28.748640Z","shell.execute_reply":"2025-05-02T10:14:00.165734Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-05-02 10:13:38.441687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746180818.639203      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746180818.693504      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Install dependencies as needed:\n# pip install kagglehub[pandas-datasets]\nimport kagglehub\nfrom kagglehub import KaggleDatasetAdapter\n\n# Set the path to the file you'd like to load\nfile_path = \"song_lyrics.csv\"\n\n# Load the latest version\ndf = kagglehub.load_dataset(\n  KaggleDatasetAdapter.PANDAS,\n  \"carlosgdcj/genius-song-lyrics-with-language-information\",\n  file_path,\n  # Provide any additional arguments like \n  # sql_query or pandas_kwargs. See the \n  # documenation for more information:\n  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n)\n\nprint(\"First 5 records:\", df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:14:00.166922Z","iopub.execute_input":"2025-05-02T10:14:00.167104Z","iopub.status.idle":"2025-05-02T10:18:07.637237Z","shell.execute_reply.started":"2025-05-02T10:14:00.167088Z","shell.execute_reply":"2025-05-02T10:18:07.636490Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1288909252.py:10: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n  df = kagglehub.load_dataset(\n","output_type":"stream"},{"name":"stdout","text":"First 5 records:                title  tag     artist  year   views  \\\n0          Killa Cam  rap    Cam'ron  2004  173166   \n1         Can I Live  rap      JAY-Z  1996  468624   \n2  Forgive Me Father  rap   Fabolous  2003    4743   \n3       Down and Out  rap    Cam'ron  2004  144404   \n4             Fly In  rap  Lil Wayne  2005   78271   \n\n                                       features  \\\n0                   {\"Cam\\\\'ron\",\"Opera Steve\"}   \n1                                            {}   \n2                                            {}   \n3  {\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}   \n4                                            {}   \n\n                                              lyrics  id language_cld3  \\\n0  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1            en   \n1  [Produced by Irv Gotti]\\n\\n[Intro]\\nYeah, hah,...   3            en   \n2  Maybe cause I'm eatin\\nAnd these bastards fien...   4            en   \n3  [Produced by Kanye West and Brian Miller]\\n\\n[...   5            en   \n4  [Intro]\\nSo they ask me\\n\"Young boy\\nWhat you ...   6            en   \n\n  language_ft language  \n0          en       en  \n1          en       en  \n2          en       en  \n3          en       en  \n4          en       en  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df = df.groupby('artist').filter(lambda x: len(x) >= 2)\ndf = df.groupby('artist').apply(lambda x: x.sample(n=2, random_state=42)).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:18:07.638110Z","iopub.execute_input":"2025-05-02T10:18:07.638387Z","iopub.status.idle":"2025-05-02T10:21:14.480205Z","shell.execute_reply.started":"2025-05-02T10:18:07.638363Z","shell.execute_reply":"2025-05-02T10:21:14.479614Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3358028032.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df = df.groupby('artist').apply(lambda x: x.sample(n=2, random_state=42)).reset_index(drop=True)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"if len(df) > 650000:\n    df = df.sample(n=650000, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:21:14.604167Z","iopub.execute_input":"2025-05-02T10:21:14.604417Z","iopub.status.idle":"2025-05-02T10:21:16.283728Z","shell.execute_reply.started":"2025-05-02T10:21:14.604393Z","shell.execute_reply":"2025-05-02T10:21:16.283151Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.drop(columns = [\"tag\",\"year\",\"views\",\"features\",\"id\",\"language_cld3\",\"language_ft\",\"language\"],inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:21:16.337392Z","iopub.execute_input":"2025-05-02T10:21:16.337619Z","iopub.status.idle":"2025-05-02T10:21:17.232303Z","shell.execute_reply.started":"2025-05-02T10:21:16.337604Z","shell.execute_reply":"2025-05-02T10:21:17.231764Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df['instruction'] = \"Generate a song in the style of \" + df['artist']\ndf['response'] = \"Title: \" + df['title'] + \"\\nLyrics:\\n\" + df['lyrics']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:21:17.413947Z","iopub.execute_input":"2025-05-02T10:21:17.414224Z","iopub.status.idle":"2025-05-02T10:21:20.872146Z","shell.execute_reply.started":"2025-05-02T10:21:17.414209Z","shell.execute_reply":"2025-05-02T10:21:20.871459Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"chat_template = \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n\n### Instruction:\n{INPUT}\n\n### Response:\n{OUTPUT}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:21:20.874949Z","iopub.execute_input":"2025-05-02T10:21:20.875142Z","iopub.status.idle":"2025-05-02T10:21:20.878582Z","shell.execute_reply.started":"2025-05-02T10:21:20.875127Z","shell.execute_reply":"2025-05-02T10:21:20.877923Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df['text'] = df.apply(lambda row: chat_template.format(INPUT=row['instruction'], OUTPUT=row['response']), axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:21:20.879313Z","iopub.execute_input":"2025-05-02T10:21:20.879513Z","iopub.status.idle":"2025-05-02T10:21:26.851411Z","shell.execute_reply.started":"2025-05-02T10:21:20.879497Z","shell.execute_reply":"2025-05-02T10:21:26.850831Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:21:26.852901Z","iopub.execute_input":"2025-05-02T10:21:26.853173Z","iopub.status.idle":"2025-05-02T10:21:27.549337Z","shell.execute_reply.started":"2025-05-02T10:21:26.853149Z","shell.execute_reply":"2025-05-02T10:21:27.548741Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_df[['text']])\neval_dataset = Dataset.from_pandas(eval_df[['text']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:21:27.550114Z","iopub.execute_input":"2025-05-02T10:21:27.550386Z","iopub.status.idle":"2025-05-02T10:21:39.965809Z","shell.execute_reply.started":"2025-05-02T10:21:27.550363Z","shell.execute_reply":"2025-05-02T10:21:39.965265Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:21:39.966757Z","iopub.execute_input":"2025-05-02T10:21:39.966971Z","iopub.status.idle":"2025-05-02T10:21:39.975875Z","shell.execute_reply.started":"2025-05-02T10:21:39.966955Z","shell.execute_reply":"2025-05-02T10:21:39.975210Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'text': 'Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\\n\\n### Instruction:\\nGenerate a song in the style of Luke Goliath x Lynxmack\\n\\n### Response:\\nTitle: Nothing\\nLyrics:\\nYou don’t gotta hate on me\\nYou don’t gotta say nothing\\nGirl I know that you love\\u2005me\\u2005(that you love\\u2005me)\\n(Love, me)\\nI mean it’s easy to\\u2005see (to see, to see)\\nThat don’t got no filter (that you don’t got no filter)\\n\\nI been focused on you\\nI been rolling for you (for you)\\nSmoking for you (for you)\\nAnd baby don’t waste no time get straight to the point (straight to the point)\\nYeah\\nI know you don’t got no filter (you don’t got no filter)\\n\\nYou just make me feel some way\\nYou just make, you just make me wanna come down tonight\\nAnd I don’t care if your nigga home tonight (if your nigga home tonight)\\nI was hoping you alone tonight\\nTryna get this work tonight\\n\\nSunrise to sundown, yeah your eyes in the light tonight (tonight)\\nYou on, you on my, on my mind\\nI really hope that you the one\\nAnd I can’t lie (and I can’t lie)\\nNo I can’t lie\\nYeah yeah\\nYou don’t gotta hate on me (on me)\\nYou don’t gotta say nothing\\nGirl I know that you love me (that you love me)\\n(Love, me)\\nI mean it’s easy to see (it’s easy to see)\\nThat don’t got no filter (that you don’t got no filter)\\n\\nI been focused on you\\nI been rolling for you (for you)\\nSmoking for you (for you)\\nAnd baby don’t waste no time get straight to the point (straight to the point)\\n(Don’t waste no time)\\nYeah\\nI know you don’t got no filter (you don’t got no filter)\\n\\nLoving the way you threw it back (threw it back)\\nLove it when you say you attached to me (you attached)\\nI wanna give you babies, love the way you lick your lips so crazy\\nAnd I - (I)\\nI’d really like some more\\nSome more of you\\n\\nIt’s only you for me\\nOnly you for me\\nIt’s you for me\\nAnd for these other girls baby it’s a -\\nIt’s a two for me\\nOnly you for me\\nNobody else for me\\nYeah\\nYou don’t gotta hate on me\\nYou don’t gotta say nothing\\nGirl I know that you love me (that you love me)\\n(Love, me)\\nI mean it’s easy to see (to see, to see)\\nThat don’t got no filter (that you don’t got no filter)\\nI been focused on you\\nI been rolling for you (for you)\\nSmoking for you (for you)\\nAnd baby don’t waste no time get straight to the point (straight to the point)\\nYeah\\nI know you don’t got no filter (you don’t got no filter)',\n '__index_level_0__': 366527}"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"compute_dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n    max_seq_length=2048,\n    dtype=compute_dtype,\n    load_in_4bit=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:21:39.976664Z","iopub.execute_input":"2025-05-02T10:21:39.976905Z","iopub.status.idle":"2025-05-02T10:23:14.847473Z","shell.execute_reply.started":"2025-05-02T10:21:39.976891Z","shell.execute_reply":"2025-05-02T10:23:14.846938Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.4.4: Fast Llama patching. Transformers: 4.51.1.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a38e8d097804f6f9bb46bb23f25ce5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"129f76834395483e95fb32132ec92f0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3729561f11694274a0116cb5dd4cc533"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b4dc43f4da2456c8c1afaa3ae675f90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eb584d6a5814ec389c50e606b264781"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=4,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=42,\n    use_rslora=False,\n    loftq_config=None,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:23:14.848190Z","iopub.execute_input":"2025-05-02T10:23:14.848368Z","iopub.status.idle":"2025-05-02T10:23:21.368091Z","shell.execute_reply.started":"2025-05-02T10:23:14.848354Z","shell.execute_reply":"2025-05-02T10:23:21.367340Z"}},"outputs":[{"name":"stderr","text":"Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\nare not enabled or a bias term (like in Qwen) is used.\nUnsloth 2025.4.4 patched 32 layers with 32 QKV layers, 32 O layers and 0 MLP layers.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:23:21.369526Z","iopub.execute_input":"2025-05-02T10:23:21.369773Z","iopub.status.idle":"2025-05-02T10:23:21.381315Z","shell.execute_reply.started":"2025-05-02T10:23:21.369756Z","shell.execute_reply":"2025-05-02T10:23:21.380606Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n        (layers): ModuleList(\n          (0): LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n          (1): LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n          (2-31): 30 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:23:21.382592Z","iopub.execute_input":"2025-05-02T10:23:21.382844Z","iopub.status.idle":"2025-05-02T10:23:21.395507Z","shell.execute_reply.started":"2025-05-02T10:23:21.382819Z","shell.execute_reply":"2025-05-02T10:23:21.394851Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# Configuration specifically optimized for Tesla T4\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = 2048,\n    dataset_num_proc = 1,\n    packing = False,\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 15,\n        learning_rate = 2e-4,\n        # T4 supports FP16 but not BF16\n        fp16 = True,     # Enable FP16 which is supported\n        bf16 = False,    # Explicitly disable BF16 which is not supported\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\",\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:23:21.396481Z","iopub.execute_input":"2025-05-02T10:23:21.396676Z","iopub.status.idle":"2025-05-02T10:29:41.888419Z","shell.execute_reply.started":"2025-05-02T10:23:21.396662Z","shell.execute_reply":"2025-05-02T10:29:41.887815Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/520000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c51ccfdce13346058e19afcde9aeda3c"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"trainer_stats = trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:29:41.889174Z","iopub.execute_input":"2025-05-02T10:29:41.889432Z","iopub.status.idle":"2025-05-02T10:39:28.607421Z","shell.execute_reply.started":"2025-05-02T10:29:41.889415Z","shell.execute_reply":"2025-05-02T10:39:28.606832Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 520,000 | Num Epochs = 1 | Total steps = 15\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 3,407,872/8,000,000,000 (0.04% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 08:47, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.170400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.098900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.643300</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.519200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.777300</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.713800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.867400</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.196700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.756000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>3.200900</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.361300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.415400</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.872300</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.738300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.341700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Save model and tokenizer\nmodel.save_pretrained(\"/kaggle/working/my_model\")\ntokenizer.save_pretrained(\"/kaggle/working/my_model\")\n\n# Compress the directory\nimport shutil\nshutil.make_archive(\"my_model\", 'zip', \"/kaggle/working/my_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:08:31.737469Z","iopub.execute_input":"2025-05-02T11:08:31.737852Z","iopub.status.idle":"2025-05-02T11:08:34.567332Z","shell.execute_reply.started":"2025-05-02T11:08:31.737828Z","shell.execute_reply":"2025-05-02T11:08:34.566616Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/my_model.zip'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from IPython.display import FileLink\ndisplay(FileLink(r'my_model.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:12:10.891298Z","iopub.execute_input":"2025-05-02T11:12:10.891995Z","iopub.status.idle":"2025-05-02T11:12:10.897573Z","shell.execute_reply.started":"2025-05-02T11:12:10.891966Z","shell.execute_reply":"2025-05-02T11:12:10.896865Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/my_model.zip","text/html":"<a href='my_model.zip' target='_blank'>my_model.zip</a><br>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}